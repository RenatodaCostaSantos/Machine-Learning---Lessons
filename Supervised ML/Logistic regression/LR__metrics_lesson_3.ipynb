{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF2tKaS1Tj/BFjNhbKcCOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RenatodaCostaSantos/Machine-Learning---Lessons/blob/main/Supervised%20ML/Logistic%20regression/LR__metrics_lesson_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics: Evaluating a logistic regression model performance\n",
        "\n",
        "So far we've learned how to instantiate a logistic regression model and how to fit it with any data. However, how do we judge if the model performance?\n",
        "\n",
        "We need a way to measure how the predicted outcomes relate to the actual outcomes themselves. The way machine learning implements it is through a metric.\n",
        "\n",
        "In this lesson, we will learn about six different types of common metrics used in logistic regression problems:\n",
        "\n",
        "- Accuracy,\n",
        "\n",
        "- Sensitivity,\n",
        "\n",
        "- Specificity,\n",
        "\n",
        "- Positive predicted value (PPV),\n",
        "\n",
        "- Negative predicted value (PPN),\n",
        "\n",
        "- F1-score.\n",
        "\n",
        "Once again we will use the auto dataset to exemplify the concepts we introduce during this lesson. Let's read the dataset:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QNJdcx3ZA9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4J7t3BXFP5I",
        "outputId": "0b67942a-56c8-477b-cbb0-8db290fd9e01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load auto dataset\n",
        "auto = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Logistic regression/automobiles.csv')"
      ],
      "metadata": {
        "id": "0a-SZWWpDigi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "0MG2AWLxF0Le",
        "outputId": "ccfa8e4d-4049-4e98-e207-1ca6e20094cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   symboling  normalized_losses  make fuel_type aspiration num_of_doors  \\\n",
              "0          2                164  audi       gas        std         four   \n",
              "1          2                164  audi       gas        std         four   \n",
              "2          1                158  audi       gas        std         four   \n",
              "3          1                158  audi       gas      turbo         four   \n",
              "4          2                192   bmw       gas        std          two   \n",
              "\n",
              "  body_style drive_wheels engine_location  wheel_base  ...  engine_size  \\\n",
              "0      sedan          fwd           front        99.8  ...          109   \n",
              "1      sedan          4wd           front        99.4  ...          136   \n",
              "2      sedan          fwd           front       105.8  ...          136   \n",
              "3      sedan          fwd           front       105.8  ...          131   \n",
              "4      sedan          rwd           front       101.2  ...          108   \n",
              "\n",
              "   fuel_system  bore  stroke compression_ratio horsepower  peak_rpm city_mpg  \\\n",
              "0         mpfi  3.19     3.4              10.0        102      5500       24   \n",
              "1         mpfi  3.19     3.4               8.0        115      5500       18   \n",
              "2         mpfi  3.19     3.4               8.5        110      5500       19   \n",
              "3         mpfi  3.13     3.4               8.3        140      5500       17   \n",
              "4         mpfi  3.50     2.8               8.8        101      5800       23   \n",
              "\n",
              "   highway_mpg  price  \n",
              "0           30  13950  \n",
              "1           22  17450  \n",
              "2           25  17710  \n",
              "3           20  23875  \n",
              "4           29  16430  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7824d82e-be56-4d57-921b-f502dc59fdf4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symboling</th>\n",
              "      <th>normalized_losses</th>\n",
              "      <th>make</th>\n",
              "      <th>fuel_type</th>\n",
              "      <th>aspiration</th>\n",
              "      <th>num_of_doors</th>\n",
              "      <th>body_style</th>\n",
              "      <th>drive_wheels</th>\n",
              "      <th>engine_location</th>\n",
              "      <th>wheel_base</th>\n",
              "      <th>...</th>\n",
              "      <th>engine_size</th>\n",
              "      <th>fuel_system</th>\n",
              "      <th>bore</th>\n",
              "      <th>stroke</th>\n",
              "      <th>compression_ratio</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>peak_rpm</th>\n",
              "      <th>city_mpg</th>\n",
              "      <th>highway_mpg</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>fwd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.8</td>\n",
              "      <td>...</td>\n",
              "      <td>109</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>102</td>\n",
              "      <td>5500</td>\n",
              "      <td>24</td>\n",
              "      <td>30</td>\n",
              "      <td>13950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>4wd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.4</td>\n",
              "      <td>...</td>\n",
              "      <td>136</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.4</td>\n",
              "      <td>8.0</td>\n",
              "      <td>115</td>\n",
              "      <td>5500</td>\n",
              "      <td>18</td>\n",
              "      <td>22</td>\n",
              "      <td>17450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>fwd</td>\n",
              "      <td>front</td>\n",
              "      <td>105.8</td>\n",
              "      <td>...</td>\n",
              "      <td>136</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.4</td>\n",
              "      <td>8.5</td>\n",
              "      <td>110</td>\n",
              "      <td>5500</td>\n",
              "      <td>19</td>\n",
              "      <td>25</td>\n",
              "      <td>17710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>turbo</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>fwd</td>\n",
              "      <td>front</td>\n",
              "      <td>105.8</td>\n",
              "      <td>...</td>\n",
              "      <td>131</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.13</td>\n",
              "      <td>3.4</td>\n",
              "      <td>8.3</td>\n",
              "      <td>140</td>\n",
              "      <td>5500</td>\n",
              "      <td>17</td>\n",
              "      <td>20</td>\n",
              "      <td>23875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>192</td>\n",
              "      <td>bmw</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>sedan</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>101.2</td>\n",
              "      <td>...</td>\n",
              "      <td>108</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.8</td>\n",
              "      <td>8.8</td>\n",
              "      <td>101</td>\n",
              "      <td>5800</td>\n",
              "      <td>23</td>\n",
              "      <td>29</td>\n",
              "      <td>16430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7824d82e-be56-4d57-921b-f502dc59fdf4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7824d82e-be56-4d57-921b-f502dc59fdf4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7824d82e-be56-4d57-921b-f502dc59fdf4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a binary autocome column\n",
        "auto['high_price'] = 0\n",
        "auto.loc[auto['price'] > 15000, 'high_price'] = 1"
      ],
      "metadata": {
        "id": "ktGA0wKnF3dn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check values for new column\n",
        "auto['high_price'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXSIMSegGMgy",
        "outputId": "05d44aec-39b9-45cb-ce51-4a8b760ca04e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    119\n",
              "1     40\n",
              "Name: high_price, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create features and target variables\n",
        "X = auto.drop(['high_price','price'], axis = 1)\n",
        "y = auto['high_price']"
      ],
      "metadata": {
        "id": "rm491MOxj3Hz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 712)"
      ],
      "metadata": {
        "id": "v1MP_YALkE0K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy\n",
        "\n",
        "The first and most important metric in classification problems is accuracy. It is defined by:\n",
        "$$\n",
        "accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Number of observations}}.\n",
        "$$\n",
        "\n",
        "In a binary problem, a correct prediction happens when the model predicts 1 when the outcome was 1 and 0 when the outcome was 0. It measures how accurate the model was. \n",
        "\n",
        "In practice, LogisticRegression class of sklearn has a [score method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score) that quickly performs the accuracy calculation (keep in mind that this method is not always available for classes of sklearn). Let's practice it using the auto dataset. We will use only the horsepower to create the first model of this lesson:\n",
        "\n"
      ],
      "metadata": {
        "id": "ijSMR2hLGaOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subset of X\n",
        "X_sub = X_train[['horsepower']]"
      ],
      "metadata": {
        "id": "lV_JxLJiJuhB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a model\n",
        "model = LogisticRegression()\n",
        "# Fit the model using X\n",
        "model.fit(X_sub,y_train)\n",
        "# Check model's accuracy\n",
        "score = model.score(X_sub,y_train)\n",
        "\n",
        "print(f'The accuracy of the model on the training set was {score*100:.2f}%.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OurbAWbqHM5S",
        "outputId": "b16b563a-2bba-4568-f5d6-f7f3d6064529"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model on the training set was 86.61%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model predicted correctly 86% of the outcomes in the training set. Evaluating a model in the training set always gives an optimistic sense of how the model is performing. However, the model learns about the data every time we train it, so the correct way to judge a model's performance is by using the test set once we are confident about the model. For that reason, one should evaluate a model only once in the test set.\n",
        "\n",
        "# Sensitivity\n",
        "\n",
        "Sometimes it is more important to have a measurement of how many predictions, among all the positive outcomes, were correctly identified by a model. That's what sensitivity does. It is defined as:\n",
        "$$\n",
        "sensitivity = \\frac{TP}{TP + FN},\n",
        "$$\n",
        "where $TP$ and $FN$ stand for true positives and false negatives respectively. True positives and false negatives are known to be positive observations, however, the true positives are positive outcomes that were correctly identified by the model, while false negatives are positive outcomes that were mistakenly predicted as a negative outcomes by the model.\n",
        "\n",
        "The image below provides a way to visualize the sensitivity:\n",
        "\n",
        "![sensitivity](https://raw.githubusercontent.com/RenatodaCostaSantos/Machine-Learning---Lessons/main/Supervised%20ML/Logistic%20regression/images/sensitivity.png)\n",
        "\n",
        "\n",
        "\n",
        "The word true means the labels of the predictions and outcomes match; both are either 0 or 1. The word false means the opposite, *i.e.*, the predictions and outcomes do not match. The words positive and negative are associated with the labels of the predictions made by the model.\n",
        "\n",
        "\n",
        "One can think of the sensitivity as a conditional probability statement. In other words, given that an outcome was positive, what is the probability of a model correctly identifying it?\n",
        "\n",
        "It is important to be aware that sensitivity is sometimes called the **recall** in machine learning literature.\n",
        "\n",
        "Let's calculate the sensitivity of the model we created using the auto dataset. We will use the predict method of the LogisticRegression class to retrive the labels of the predictions:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mkfitQC2KMbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of true positives\n",
        "tp = sum( (model.predict(X_sub) == 1) & (y_train ==1))\n",
        "print(tp)\n",
        "# Find the number of false negatives\n",
        "fn = sum( (model.predict(X_sub) == 0) & (y_train == 1))\n",
        "print(fn)\n",
        "# Calculate sensitivity\n",
        "sensitivity = tp / (tp+fn)\n",
        "\n",
        "print(f'The sensitivity of the model is {sensitivity*100:.2f}%.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LUem05eLQmQ",
        "outputId": "b26b129a-cfa8-4ff9-8d8a-5f74cede20fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "14\n",
            "The sensitivity of the model is 58.82%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sensitivity answers how many of the positive outcomes were correctly predicted by a model/test. However, sometimes it is more useful to know if a prediction is likely to agree with the outcome. That leads us to the next metric which is related to sensitivity.\n",
        "\n",
        "# Negative predictive value (NPV)\n",
        "\n",
        "Imagine a common real-life scenario: \n",
        "\n",
        "- A patient who tested negative was told the test used had high sensitivity. Should she trust the prediction of the test and go home relieved?\n",
        "\n",
        "She is not sure and decides to ask her doctor. He, as an average doctor, gives her a complex answer:\n",
        "\n",
        "- A negative prediction in a high-sensitivity test is useful to **rule out** a **positive outcome**. \n",
        "\n",
        "According to the sentence above, she is very likely not to be sick and should go home relieved. That's what doctors usually do; they say a beautiful long sentence and send you home. But she is a bit stubborn and wants to understand why she should be relieved. Let's consider an example to help her. Suppose a test has 90,9% sensitivity. That means, if the test was tried on 11 sick patients, it correctly identified 10 of them. It rarely missed sick patients. You can visualize it by focusing on the grey circle of the figure below:\n",
        "\n",
        "![NPV](https://raw.githubusercontent.com/RenatodaCostaSantos/Machine-Learning---Lessons/main/Supervised%20ML/Logistic%20regression/images/npv.png)\n",
        "\n",
        "Now, this does not explain why she should be relieved after receiving a negative result from the high-sensitivity test. The reason why she can be relieved can be visualized by the orange circle of the figure above. It answers the following conditional probability question:\n",
        "\n",
        "Given that this high-sensitivity **test prediction was negative**, what is the probability that the **patient is not sick**? In other words, what is $P(\\text{outcome} = 0 \\, | \\, \\text{prediction} = 0)$ for this test? \n",
        "\n",
        "As we can see from the figure, $P(\\text{outcome} = 0 \\, | \\, \\text{prediction} = 0) = 88,8 \\%$, and if the patient got a negative result, she now has a good reason to be relieved (in practice one would like an even higher probability).\n",
        "\n",
        "An intuitive way of thinking about it also comes from the drawing above. A high-sensitivity test leaves very few false negatives (upper left-hand corner of the figure). That makes a negative prediction very likely to be correct given that, from all negative predictions, very few of them are false negatives in a high-sensitivity test.\n",
        "\n",
        "This conditional probability we just described is called **negative predictive value** or NPV. We can write it as:\n",
        "$$\n",
        "NPV = P(\\text{outcome} = 0 \\, | \\, \\text{prediction} = 0) = \\frac{TN}{TN + FN}\n",
        "$$\n",
        "\n",
        "Note that it relies on the true negative outcomes, so it has to be computed on the training set. However, it answers a different question than the sensitivity metric. It estimates how likely a patient is not sick, given that she tested negative. \n",
        "\n",
        "A high-sensitivity model **does not rule in positive outcomes**, though. To rule in a positive result for a test, we should have a high-probability answer for the following question:\n",
        "\n",
        "Given that a patient tested positive in a high-sensitivity test, what is the probability that he is sick?\n",
        "\n",
        "By focusing on the blue circle of the image above, we can see that the answer to this question is $P(\\text{outcome} = 1 \\, | \\, \\text{prediction} = 1) = 58,8 \\%$, which is not very high. That's because the definition of sensitivity does not consider false positives at all! However, they will impact the probability of a positive prediction agreeing with the outcome.\n",
        "\n",
        "Let's calculate the negative predictive value for the model we have. Since the sensitivity for this model was not good, we do not know what to expect for the NPV. Let's check out its value:\n",
        "\n"
      ],
      "metadata": {
        "id": "UdxMviUjsLQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute true negatives\n",
        "tn = sum((y_train == 0) & (model.predict(X_sub) == 0))\n",
        "print(tn)\n",
        "# Compute false negatives\n",
        "fn = sum((y_train == 1) & (model.predict(X_sub) == 0))\n",
        "print(fn)\n",
        "# Calculate negative predictive value\n",
        "npv = tn/(tn+fn)\n",
        "\n",
        "print(f'The NPV of the model is {npv*100:.2f}%.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrxHKZpCx2qQ",
        "outputId": "1969869b-57d0-4c50-d758-e4b3395ef791"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "14\n",
            "The NPV of the model is 86.54%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the sensitivity was not high, the NPV was reasonably good, and negative predicted values by this model are likely to be negative outcomes."
      ],
      "metadata": {
        "id": "Gyap6hUd0QCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specificity\n",
        "\n",
        "In some cases, it is more important to check if negative outcomes are correctly predicted by a model. None of the metrics above provides that information. That is the role of specificity, which is defined by:\n",
        "$$\n",
        "specificity = \\frac{TN}{TN + FP},\n",
        "$$\n",
        "where $TN$ and $FP$ stand for true negatives and false positives respectively. True negatives and false positives are known to be negative outcomes. However, true negatives are negative outcomes correctly predicted by the model, while false positives are negative outcomes mistakenly predicted as positive outcomes by the model.\n",
        "\n",
        "The image below provides a way to visualize the specificity:\n",
        "\n",
        "![specificity](https://raw.githubusercontent.com/RenatodaCostaSantos/Machine-Learning---Lessons/main/Supervised%20ML/Logistic%20regression/images/specificity.png)\n",
        "\n",
        "One can think of the specificity as a conditional probability statement. In other words, given that an outcome was negative, what is the probability of a model correctly identifying it?\n",
        "\n",
        "\n",
        "Let's calculate the specificity of the model we created using the auto dataset. We will use the predict method of the LogisticRegression class to retrive the labels of the predictions:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U4QzAHxN150P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute true negatives\n",
        "tn = sum((y_train == 0) & (model.predict(X_sub) == 0))\n",
        "print(tn)\n",
        "# Compute false positives\n",
        "fp = sum((y_train == 0) & (model.predict(X_sub) == 1))\n",
        "print(fp)\n",
        "\n",
        "# Calculate specificity\n",
        "specificity = tn/(tn+fp)\n",
        "\n",
        "print(f'The specificity of the model is {specificity*100:.2f}%.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lltcKy032nI",
        "outputId": "4784f3ac-699f-428f-deec-919b2218935d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "3\n",
            "The specificity of the model is 96.77%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has a high specificity (at least in the training set). That means, among all negative outcomes, most of them were correctly predicted by the model.\n",
        "\n",
        "Specificity answers how many of the negative outcomes were correctly predicted by a model/test. However, sometimes it is more useful to know if a prediction is likely to agree with the outcome. We already saw that the NPV provides the answer for the negative predictions. For the positive predictions we are led to the next metric which is related to specificity."
      ],
      "metadata": {
        "id": "D5ZfKXZi4Hq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Positive predictive value (PPN)\n",
        "\n",
        "Once again, imagine the common real-life scenario: \n",
        "\n",
        "- A patient who **tested positive** was told the test used had high specificity. Should she trust the prediction of the test and be worried?\n",
        "\n",
        "She is not sure and decides to ask her doctor. He, as an average doctor, gives her a complex answer:\n",
        "\n",
        "- A positive prediction in a high-specificity test is useful to **rule out** a **negative outcome**. \n",
        "\n",
        "According to the sentence above, she is very likely to be sick and should worry. That's what doctors usually do; they say a dreadful long sentence and send you home. But she is a bit stubborn and wants to understand why she should worry. Let's consider an example to help her accept this result. Suppose a test has 92,3% specificity. That means, if the test was tried on 13 healthy patients, it correctly identified 12 of them. It rarely missed healthy patients. You can visualize it by focusing on the purple circle of the figure below:\n",
        "\n",
        "![PPV](https://raw.githubusercontent.com/RenatodaCostaSantos/Machine-Learning---Lessons/main/Supervised%20ML/Logistic%20regression/images/ppv.png)\n",
        "\n",
        "\n",
        "Now, this does not explain why the patient above should be worried once she got a positive result from the high-specificity test. The reason why she could start worrying can be visualized in the blue circle of the figure above. It answers the following conditional probability question:\n",
        "\n",
        "Given that this high-specificity **test prediction was positive**, what is the probability that the **patient is sick**? In other words, what is $P(\\text{outcome} = 1| \\text{prediction} = 1)$ for this test? \n",
        "\n",
        "As we can see from the figure, $P(\\text{outcome} = 1| \\text{prediction} = 1) = 92,3 \\%$, and if the patient got a positive result, her worries would be justified (in practice one would like an even higher probability).\n",
        "\n",
        "An intuitive way of thinking about it also comes also from the drawing above. A high-specificity test leaves very few false positives (bottom right-hand corner of the figure). This is what makes a positive prediction very likely to agree with the outcome given that, from all positive predictions, very few of them are false positives in a high-specificity test.\n",
        "\n",
        "This conditional probability we just described is called **positive predictive value** or PPV. We can write it as:\n",
        "$$\n",
        "PPV = P(\\text{outcome} = 1 \\, | \\, \\text{prediction} = 1) = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "Note that it relies on the true positive outcomes, so it has to be computed on the training set. However, it answers a different question than the specificity metric. It estimates how likely a patient is sick, given that she tested positive. \n",
        "\n",
        "It is important to be aware that the PPV is also called the **precision** in machine learning literature.\n",
        "\n",
        "\n",
        "A high-specificity model/test **does not rule in negative outcomes**, though. To rule in a negative result for a test, we would have to have a high-probability answer for the following question:\n",
        "\n",
        "Given that a patient tested negative in a high-specificity test, what is the probability that he is not sick?\n",
        "\n",
        "By focusing on the dark red circle of the image above we can see that the answer to this question is $P(\\text{outcome} = 0 \\, | \\, \\text{prediction} = 0) = 52,4 \\%$, which is not very high. That's because the definition of specificity does not consider false negatives at all! However, they will impact the probability of a negative prediction agreeing with the outcome.\n",
        "\n",
        "Let's calculate the positive predictive value for the model we have. Since the specificity for this model was very good, we do expect a high probability for the PPV. Let's check out this prediction:\n"
      ],
      "metadata": {
        "id": "N3D2PmZm3xrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of true positives\n",
        "tp = sum((model.predict(X_sub) == 1) & (y_train == 1))\n",
        "print(tp)\n",
        "# Compute the number of false positives\n",
        "fp = sum((model.predict(X_sub) == 1) & (y_train == 0))\n",
        "print(fp)\n",
        "\n",
        "ppv = tp/(tp+fp)\n",
        "\n",
        "print(f'The positive predictive value of the model is {ppv*100:.2f}%.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wJgf8w82zU3",
        "outputId": "d67c888f-acfc-4d13-bed4-13868b56175d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "3\n",
            "The positive predictive value of the model is 86.96%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the PPV is high, meaning that positive predicted values by this model are likely to be positive outcomes."
      ],
      "metadata": {
        "id": "7VMsq4KtF9nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1-score\n",
        "\n",
        "Sometimes, for example in medical classification problems, it is crucial to predict positive cases correctly. A model/test that is able to do that can save lives. Thus, an important question arises: How can we tell that a model/test is good at predicting positive cases correctly?\n",
        "\n",
        "The answer to that question is not straightforward. There are essentially two sides to the story.\n",
        "\n",
        "1 - We need to be able to check if the model/test is good at selecting sick patients. For that, we need to apply/train the test/model in patients that are known to be sick and check how many of them were correctly identified. That's the role of **recall** or **sensitivity**, as one can see in the image below:\n",
        "\n",
        "![high_recall](https://raw.githubusercontent.com/RenatodaCostaSantos/Machine-Learning---Lessons/main/Supervised%20ML/Logistic%20regression/images/high_recall_avg_precision.png)\n",
        "\n",
        "As we can see, recall is a metric that informs if a test/model is very good at identifying sick patients once we **know** that the patients were sick. It provides information about the model/test, but no extra information about the patients (we already knew the patients' condition).\n",
        "\n",
        "2 - We have to know if a patient who tested positive is indeed sick. For that, we apply the test/model on patients without knowing their condition and compare the predictions with the true outcomes. That's what the **precision** or **positive predictive value** does.\n",
        "\n",
        "![high_precision](https://raw.githubusercontent.com/RenatodaCostaSantos/Machine-Learning---Lessons/main/Supervised%20ML/Logistic%20regression/images/high_precision_avg_recall.png)\n",
        "\n",
        "\n",
        "The images above show that a test/model with high sensitivity does not imply it will have high precision. Also, a model/test with high precision does not imply high sensitivity.\n",
        "\n",
        "To be sure we are correctly identifying and communicating our findings to patients, we need a metric that will return a high number only when a test/model has high recall and high precision. This is the scenario shown in the image below:\n",
        "\n",
        "![great_model](https://raw.githubusercontent.com/RenatodaCostaSantos/Machine-Learning---Lessons/main/Supervised%20ML/Logistic%20regression/images/high_precision_high_recall.png)\n",
        "\n",
        "For that, we need a metric that takes into consideration the precision and the recall. That's what the f1-score does. It is defined as:\n",
        "$$\n",
        "\\text{f1-score} = \\frac{2*p * r}{p + r}\n",
        "$$\n",
        "where, $p$, is the precision (also known as PPV) and $r$ is the recall (also known as sensitivity).\n",
        "\n",
        "The f1-score has two important properties:\n",
        "\n",
        "- It lies between the precision and recall values.\n",
        "\n",
        "- It is never greater than the arithmetic mean of the precision and the recall. The numerical value is weighted towards the minimum of the $p$ and $r$ values, only equal to the arithmetic mean when $p=r$.\n",
        "\n",
        "The f1-score is a better metric to classify positive outcomes reliably. It returns a value between the precision and the recall, leaning towards the worse of those values. If the f1-score is high, we can be more confident to say in one sentence that the model/test is good at selecting sick patients **and**, once a patient tested positive, it is very likely he/she is, in fact, sick."
      ],
      "metadata": {
        "id": "zp26f2JmF2dE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "In this lesson we learned about the following metrics:\n",
        "\n",
        "- Accuracy\n",
        "\n",
        "- Sensitivity\n",
        "\n",
        "- Specificity\n",
        "\n",
        "- Positive predictive value\n",
        "\n",
        "- Negative predictive value\n",
        "\n",
        "- F1-score\n",
        "\n",
        "\n",
        "We also presented examples and visualizations to clarify the concepts above, created a logistic regression model, and used it to calculate the values of five of those metrics. \n",
        "\n"
      ],
      "metadata": {
        "id": "jxoC9aBI3gZU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPAHyK6wh01J"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}